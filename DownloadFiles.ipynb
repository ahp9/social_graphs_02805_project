{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf6b411d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\astri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\astri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\astri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\astri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\astri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\astri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Standard Library ---\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import html\n",
    "import random\n",
    "import pathlib\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# --- Third-party libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "import networkx as nx\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# --- NLP / Text ---\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords, words, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- NLTK Downloads ---\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3c10ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will save the text about each performer as a txt file in the folder \"performers\"\n",
    "save_dir = \"managers\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "save_dir_career = \"managers_career\"\n",
    "os.makedirs(save_dir_career, exist_ok=True)\n",
    "\n",
    "# wiki api\n",
    "baseurl = \"https://en.wikipedia.org/w/api.php\"\n",
    "user_agent = \"PremierLeagueManagerDownloader/1.0\"\n",
    "\n",
    "def get_category_members(category_title, depth=1):\n",
    "    \"\"\"\n",
    "    Fetch all pages (and optionally subcategories) under a Wikipedia category.\n",
    "    depth=1 means include one level of subcategories.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Skip Premier League Manager...... and List of Premier League managers\n",
    "    members = []\n",
    "    cmcontinue = \"\"\n",
    "    while True:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"categorymembers\",\n",
    "            \"cmtitle\": f\"Category:{category_title}\",\n",
    "            \"cmlimit\": \"max\",\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        if cmcontinue:\n",
    "            params[\"cmcontinue\"] = cmcontinue\n",
    "\n",
    "        query = baseurl + \"?\" + urllib.parse.urlencode(params)\n",
    "        req = urllib.request.Request(query, headers={\"User-Agent\": user_agent})\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            data = json.loads(response.read().decode(\"utf-8\"))\n",
    "\n",
    "        pages = data.get(\"query\", {}).get(\"categorymembers\", [])\n",
    "        for p in pages:\n",
    "            ns = p.get(\"ns\")\n",
    "            title = p.get(\"title\")\n",
    "            if ns == 0:\n",
    "                members.append(title)\n",
    "            elif ns == 14 and depth > 0:  # category namespace\n",
    "                print(f\"→ Exploring subcategory: {title}\")\n",
    "                members += get_category_members(title.replace(\"Category:\", \"\"), depth=depth - 1)\n",
    "\n",
    "        if \"continue\" in data:\n",
    "            cmcontinue = data[\"continue\"][\"cmcontinue\"]\n",
    "            time.sleep(0.3)\n",
    "        else:\n",
    "            break\n",
    "    return members\n",
    "\n",
    "def fetch_wikitext(title):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    query = baseurl + \"?\" + urllib.parse.urlencode(params)\n",
    "    req = urllib.request.Request(query, headers={\"User-Agent\": user_agent})\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        data = json.loads(response.read().decode(\"utf-8\"))\n",
    "\n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    page = next(iter(pages.values()))\n",
    "    try:\n",
    "        return page[\"revisions\"][0][\"slots\"][\"main\"][\"*\"]\n",
    "    except (KeyError, IndexError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca4545ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching managers from category: Premier League managers\n",
      "→ Exploring subcategory: Category:Lists of Premier League managers\n",
      "Found 291 manager pages\n"
     ]
    }
   ],
   "source": [
    "# --- MAIN EXECUTION ---\n",
    "category = \"Premier League managers\"\n",
    "print(f\"Fetching managers from category: {category}\")\n",
    "managers = get_category_members(category, depth=1)\n",
    "print(f\"Found {len(managers)} manager pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "687a4cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "KEEP_SECTIONS = [\n",
    "    \"managerial career\",\n",
    "    \"manager career\",\n",
    "    \"coaching career\",\n",
    "    \"coaching\",\n",
    "    \"management\",\n",
    "    \"managerial\",\n",
    "    \"style of management\",\n",
    "    \"style of play\",\n",
    "    \"tactical\",\n",
    "    \"tactics\",\n",
    "    \"managerial statistics\",\n",
    "    \"statistics\",\n",
    "]\n",
    "\n",
    "REMOVE_SECTIONS = [\n",
    "    \"early life\",\n",
    "    \"youth career\",\n",
    "    \"playing career\",\n",
    "    \"international career\",\n",
    "    \"personal life\",\n",
    "    \"media\",\n",
    "    \"honours\",\n",
    "    \"legacy\",\n",
    "    \"post-retirement\",\n",
    "    \"other work\",\n",
    "    \"outside football\",\n",
    "]\n",
    "\n",
    "def filter_manager_wikitext(raw):\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "\n",
    "    # Split into headers and content\n",
    "    parts = re.split(r\"(==[^=].*?==)\", raw)\n",
    "\n",
    "    cleaned = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(parts):\n",
    "        chunk = parts[i]\n",
    "\n",
    "        # Only process level-2 headers (==Header==)\n",
    "        if re.match(r\"^==[^=].*?==$\", chunk.strip()):\n",
    "            header = chunk.strip()\n",
    "            header_name = re.sub(r\"=+\", \"\", header).strip().lower()\n",
    "\n",
    "            # Skip remove sections\n",
    "            if any(bad in header_name for bad in REMOVE_SECTIONS):\n",
    "                i += 2\n",
    "                continue\n",
    "\n",
    "            # Keep section if it matches ANY keep keyword\n",
    "            if any(good in header_name for good in KEEP_SECTIONS):\n",
    "                # Take everything until the NEXT ==...== header\n",
    "                section_text = []\n",
    "\n",
    "                # Start reading the content after this header\n",
    "                j = i + 1\n",
    "                while j < len(parts):\n",
    "                    # Stop if we hit another level-2 header\n",
    "                    if re.match(r\"^==[^=].*?==$\", parts[j].strip()):\n",
    "                        break\n",
    "                    section_text.append(parts[j])\n",
    "                    j += 1\n",
    "\n",
    "                cleaned.append(header)\n",
    "                cleaned.append(\"\".join(section_text))\n",
    "\n",
    "                i = j\n",
    "                continue\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return \"\\n\".join(cleaned).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5cb3d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_section(raw, section_name):\n",
    "    \"\"\"\n",
    "    Extract all text under == section_name == until the next ==...== header.\n",
    "    Includes all === ... === subsections.\n",
    "    \"\"\"\n",
    "    header_pattern = r\"^==\\s*([^=]+?)\\s*==\\s*$\"\n",
    "    lines = raw.split(\"\\n\")\n",
    "\n",
    "    in_target = False\n",
    "    collected = []\n",
    "\n",
    "    for line in lines:\n",
    "        m = re.match(header_pattern, line.strip())\n",
    "        if m:\n",
    "            current_header = m.group(1).strip().lower()\n",
    "\n",
    "            # Start collecting\n",
    "            if current_header == section_name.lower():\n",
    "                in_target = True\n",
    "                continue\n",
    "\n",
    "            # If another section appears → stop\n",
    "            if in_target:\n",
    "                break\n",
    "\n",
    "        if in_target:\n",
    "            collected.append(line)\n",
    "\n",
    "    return \"\\n\".join(collected).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f55b197",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager \u001b[38;5;129;01min\u001b[39;00m managers:\n\u001b[0;32m      2\u001b[0m     safe_title \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     wikitext \u001b[38;5;241m=\u001b[39m fetch_wikitext(manager)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wikitext:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanager\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, no text found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 65\u001b[0m, in \u001b[0;36mfetch_wikitext\u001b[1;34m(title)\u001b[0m\n\u001b[0;32m     63\u001b[0m query \u001b[38;5;241m=\u001b[39m baseurl \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39murlencode(params)\n\u001b[0;32m     64\u001b[0m req \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(query, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_agent})\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(req) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m     66\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     68\u001b[0m pages \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\astri\\anaconda3\\Lib\\urllib\\request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32mc:\\Users\\astri\\anaconda3\\Lib\\urllib\\request.py:515\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    512\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    514\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 515\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(req, data)\n\u001b[0;32m    517\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    518\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\astri\\anaconda3\\Lib\\urllib\\request.py:532\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    531\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 532\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_open, protocol, protocol \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    533\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m'\u001b[39m, req)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\astri\\anaconda3\\Lib\\urllib\\request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\astri\\anaconda3\\Lib\\urllib\\request.py:1392\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_open(http\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mHTTPSConnection, req,\n\u001b[0;32m   1393\u001b[0m                         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context)\n",
      "File \u001b[1;32mc:\\Users\\astri\\anaconda3\\Lib\\urllib\\request.py:1344\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1343\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1344\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[0;32m   1345\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m   1346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m   1347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n",
      "File \u001b[1;32mc:\\Users\\astri\\anaconda3\\Lib\\http\\client.py:1336\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{}, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   1334\u001b[0m             encode_chunked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\astri\\anaconda3\\Lib\\http\\client.py:1382\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[0;32m   1381\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders(body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\astri\\anaconda3\\Lib\\http\\client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\astri\\anaconda3\\Lib\\http\\client.py:1091\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1089\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer)\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1091\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1094\u001b[0m \n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\astri\\anaconda3\\Lib\\http\\client.py:1035\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m-> 1035\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1037\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "File \u001b[1;32mc:\\Users\\astri\\anaconda3\\Lib\\http\\client.py:1477\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1475\u001b[0m     server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m-> 1477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mwrap_socket(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock,\n\u001b[0;32m   1478\u001b[0m                                       server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname)\n",
      "File \u001b[1;32mc:\\Users\\astri\\anaconda3\\Lib\\ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msslsocket_class\u001b[38;5;241m.\u001b[39m_create(\n\u001b[0;32m    456\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    457\u001b[0m         server_side\u001b[38;5;241m=\u001b[39mserver_side,\n\u001b[0;32m    458\u001b[0m         do_handshake_on_connect\u001b[38;5;241m=\u001b[39mdo_handshake_on_connect,\n\u001b[0;32m    459\u001b[0m         suppress_ragged_eofs\u001b[38;5;241m=\u001b[39msuppress_ragged_eofs,\n\u001b[0;32m    460\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    461\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    462\u001b[0m         session\u001b[38;5;241m=\u001b[39msession\n\u001b[0;32m    463\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\astri\\anaconda3\\Lib\\ssl.py:1042\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m   1040\u001b[0m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1042\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\astri\\anaconda3\\Lib\\ssl.py:1320\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[0;32m   1319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for manager in managers:\n",
    "    safe_title = manager.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    wikitext = fetch_wikitext(manager)\n",
    "    \n",
    "    if not wikitext:\n",
    "        print(f\"Skipping {manager}, no text found\")\n",
    "        continue\n",
    "\n",
    "    # Use the new KEEP/REMOVE + nested subsections filter\n",
    "    filtered = filter_manager_wikitext(wikitext)\n",
    "\n",
    "    filename = os.path.join(save_dir_career, f\"{safe_title}.txt\")\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(filtered)\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5d13370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Micky Adams → managers\\Micky_Adams.txt\n",
      "Saved Neil Adams (footballer) → managers\\Neil_Adams_(footballer).txt\n",
      "Saved Tony Adams → managers\\Tony_Adams.txt\n",
      "Saved Nigel Adkins → managers\\Nigel_Adkins.txt\n",
      "Saved Dick Advocaat → managers\\Dick_Advocaat.txt\n",
      "Saved Steve Agnew → managers\\Steve_Agnew.txt\n",
      "Saved Sam Allardyce → managers\\Sam_Allardyce.txt\n",
      "Saved Clive Allen → managers\\Clive_Allen.txt\n",
      "Saved Ruben Amorim → managers\\Ruben_Amorim.txt\n",
      "Saved Carlo Ancelotti → managers\\Carlo_Ancelotti.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(wikitext)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanager\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# saves the texts in the performers folder\n",
    "for manager in managers:\n",
    "    safe_title = manager.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    wikitext = fetch_wikitext(manager)\n",
    "    if not wikitext:\n",
    "        print(f\"Skipping {manager}, no text found\")\n",
    "        continue\n",
    "\n",
    "    filename = os.path.join(save_dir, f\"{safe_title}.txt\")\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(wikitext)\n",
    "\n",
    "    print(f\"Saved {manager} → {filename}\")\n",
    "    time.sleep(0.5)  # be polite to Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eba942",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [\n",
    "    \"Glossary of association football terms\",\n",
    "    \"Association football tactics and skills\",\n",
    "    \"Formation (association football)\",\n",
    "    \"Association football positions\",\n",
    "    \"Association football\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd36d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder to save pages\n",
    "save_dir = \"football_pages\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Wikipedia API setup\n",
    "baseurl = \"https://en.wikipedia.org/w/api.php\"\n",
    "user_agent = \"FootballPageDownloader/1.0\"\n",
    "# \n",
    "def fetch_wikitext(title):\n",
    "    \"\"\"Fetch full Wikipedia page content (wikitext) for a given title.\"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    query = baseurl + \"?\" + urllib.parse.urlencode(params)\n",
    "    req = urllib.request.Request(query, headers={\"User-Agent\": user_agent})\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        data = json.loads(response.read().decode(\"utf-8\"))\n",
    "\n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    page = next(iter(pages.values()))\n",
    "    try:\n",
    "        return page[\"revisions\"][0][\"slots\"][\"main\"][\"*\"]\n",
    "    except (KeyError, IndexError):\n",
    "        return None\n",
    "\n",
    "def save_txt(title, folder):\n",
    "    \"\"\"Fetch a Wikipedia page and save it as a text file in the given folder.\"\"\"\n",
    "    text = fetch_wikitext(title)\n",
    "    if not text:\n",
    "        print(f\"⚠️ Skipped {title}: no text found.\")\n",
    "        return\n",
    "    safe_name = title.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
    "    path = os.path.join(folder, f\"{safe_name}.txt\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(f\"✅ Saved {title}\")\n",
    "\n",
    "# Example: list of mentality / tactical / philosophy pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f9a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and save all pages\n",
    "for title in pages:\n",
    "    save_txt(title, save_dir)\n",
    "    time.sleep(0.5)  # be kind to the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e4f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"football_terminology\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "baseurl = \"https://en.wikipedia.org/w/api.php\"\n",
    "user_agent = \"FootballTerminologyDownloader/1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55925d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fetch category pages ---\n",
    "def get_category_members(category_title, depth=0):\n",
    "    \"\"\"Fetch all pages (and optionally subcategories) under a Wikipedia category.\"\"\"\n",
    "    members = []\n",
    "    cmcontinue = \"\"\n",
    "    while True:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"categorymembers\",\n",
    "            \"cmtitle\": f\"Category:{category_title}\",\n",
    "            \"cmlimit\": \"max\",\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        if cmcontinue:\n",
    "            params[\"cmcontinue\"] = cmcontinue\n",
    "\n",
    "        query = baseurl + \"?\" + urllib.parse.urlencode(params)\n",
    "        req = urllib.request.Request(query, headers={\"User-Agent\": user_agent})\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            data = json.loads(response.read().decode(\"utf-8\"))\n",
    "\n",
    "        pages = data.get(\"query\", {}).get(\"categorymembers\", [])\n",
    "        for p in pages:\n",
    "            ns = p.get(\"ns\")\n",
    "            title = p.get(\"title\")\n",
    "            if ns == 0:\n",
    "                members.append(title)\n",
    "            elif ns == 14 and depth > 0:\n",
    "                print(f\"→ Exploring subcategory: {title}\")\n",
    "                members += get_category_members(title.replace(\"Category:\", \"\"), depth - 1)\n",
    "\n",
    "        if \"continue\" in data:\n",
    "            cmcontinue = data[\"continue\"][\"cmcontinue\"]\n",
    "            time.sleep(0.3)\n",
    "        else:\n",
    "            break\n",
    "    return members\n",
    "\n",
    "\n",
    "# --- Fetch full page text ---\n",
    "def fetch_wikitext(title):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    query = baseurl + \"?\" + urllib.parse.urlencode(params)\n",
    "    req = urllib.request.Request(query, headers={\"User-Agent\": user_agent})\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        data = json.loads(response.read().decode(\"utf-8\"))\n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    page = next(iter(pages.values()))\n",
    "    try:\n",
    "        return page[\"revisions\"][0][\"slots\"][\"main\"][\"*\"]\n",
    "    except (KeyError, IndexError):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63984fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"Association football terminology\"\n",
    "pages = get_category_members(category, depth=1)  # include subcategories too\n",
    "\n",
    "print(f\"Found {len(pages)} pages in category '{category}'.\")\n",
    "\n",
    "for title in pages:\n",
    "    save_txt(title, save_dir)\n",
    "    time.sleep(0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
