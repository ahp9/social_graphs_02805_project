{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6b411d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\astri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\astri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\astri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\astri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\astri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\astri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Standard Library ---\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import html\n",
    "import random\n",
    "import pathlib\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# --- Third-party libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "import networkx as nx\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# --- NLP / Text ---\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords, words, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- NLTK Downloads ---\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c10ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will save the text about each performer as a txt file in the folder \"performers\"\n",
    "save_dir = \"managers\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# wiki api\n",
    "baseurl = \"https://en.wikipedia.org/w/api.php\"\n",
    "user_agent = \"PremierLeagueManagerDownloader/1.0\"\n",
    "\n",
    "def get_category_members(category_title, depth=1):\n",
    "    \"\"\"\n",
    "    Fetch all pages (and optionally subcategories) under a Wikipedia category.\n",
    "    depth=1 means include one level of subcategories.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Skip Premier League Manager...... and List of Premier League managers\n",
    "    members = []\n",
    "    cmcontinue = \"\"\n",
    "    while True:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"categorymembers\",\n",
    "            \"cmtitle\": f\"Category:{category_title}\",\n",
    "            \"cmlimit\": \"max\",\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        if cmcontinue:\n",
    "            params[\"cmcontinue\"] = cmcontinue\n",
    "\n",
    "        query = baseurl + \"?\" + urllib.parse.urlencode(params)\n",
    "        req = urllib.request.Request(query, headers={\"User-Agent\": user_agent})\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            data = json.loads(response.read().decode(\"utf-8\"))\n",
    "\n",
    "        pages = data.get(\"query\", {}).get(\"categorymembers\", [])\n",
    "        for p in pages:\n",
    "            ns = p.get(\"ns\")\n",
    "            title = p.get(\"title\")\n",
    "            if ns == 0:\n",
    "                members.append(title)\n",
    "            elif ns == 14 and depth > 0:  # category namespace\n",
    "                print(f\"→ Exploring subcategory: {title}\")\n",
    "                members += get_category_members(title.replace(\"Category:\", \"\"), depth=depth - 1)\n",
    "\n",
    "        if \"continue\" in data:\n",
    "            cmcontinue = data[\"continue\"][\"cmcontinue\"]\n",
    "            time.sleep(0.3)\n",
    "        else:\n",
    "            break\n",
    "    return members\n",
    "\n",
    "def fetch_wikitext(title):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    query = baseurl + \"?\" + urllib.parse.urlencode(params)\n",
    "    req = urllib.request.Request(query, headers={\"User-Agent\": user_agent})\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        data = json.loads(response.read().decode(\"utf-8\"))\n",
    "\n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    page = next(iter(pages.values()))\n",
    "    try:\n",
    "        return page[\"revisions\"][0][\"slots\"][\"main\"][\"*\"]\n",
    "    except (KeyError, IndexError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4545ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAIN EXECUTION ---\n",
    "category = \"Premier League managers\"\n",
    "print(f\"Fetching managers from category: {category}\")\n",
    "managers = get_category_members(category, depth=1)\n",
    "print(f\"Found {len(managers)} manager pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d13370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the texts in the performers folder\n",
    "for manager in managers:\n",
    "    safe_title = manager.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    wikitext = fetch_wikitext(manager)\n",
    "    if not wikitext:\n",
    "        print(f\"Skipping {manager}, no text found\")\n",
    "        continue\n",
    "\n",
    "    filename = os.path.join(save_dir, f\"{safe_title}.txt\")\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(wikitext)\n",
    "\n",
    "    print(f\"Saved {manager} → {filename}\")\n",
    "    time.sleep(0.5)  # be polite to Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eba942",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [\n",
    "    \"Glossary of association football terms\",\n",
    "    \"Association football tactics and skills\",\n",
    "    \"Formation (association football)\",\n",
    "    \"Association football positions\",\n",
    "    \"Association football\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd36d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder to save pages\n",
    "save_dir = \"football_pages\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Wikipedia API setup\n",
    "baseurl = \"https://en.wikipedia.org/w/api.php\"\n",
    "user_agent = \"FootballPageDownloader/1.0\"\n",
    "# \n",
    "def fetch_wikitext(title):\n",
    "    \"\"\"Fetch full Wikipedia page content (wikitext) for a given title.\"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    query = baseurl + \"?\" + urllib.parse.urlencode(params)\n",
    "    req = urllib.request.Request(query, headers={\"User-Agent\": user_agent})\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        data = json.loads(response.read().decode(\"utf-8\"))\n",
    "\n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    page = next(iter(pages.values()))\n",
    "    try:\n",
    "        return page[\"revisions\"][0][\"slots\"][\"main\"][\"*\"]\n",
    "    except (KeyError, IndexError):\n",
    "        return None\n",
    "\n",
    "def save_txt(title, folder):\n",
    "    \"\"\"Fetch a Wikipedia page and save it as a text file in the given folder.\"\"\"\n",
    "    text = fetch_wikitext(title)\n",
    "    if not text:\n",
    "        print(f\"⚠️ Skipped {title}: no text found.\")\n",
    "        return\n",
    "    safe_name = title.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
    "    path = os.path.join(folder, f\"{safe_name}.txt\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(f\"✅ Saved {title}\")\n",
    "\n",
    "# Example: list of mentality / tactical / philosophy pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f9a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and save all pages\n",
    "for title in pages:\n",
    "    save_txt(title, save_dir)\n",
    "    time.sleep(0.5)  # be kind to the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e4f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"football_terminology\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "baseurl = \"https://en.wikipedia.org/w/api.php\"\n",
    "user_agent = \"FootballTerminologyDownloader/1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55925d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fetch category pages ---\n",
    "def get_category_members(category_title, depth=0):\n",
    "    \"\"\"Fetch all pages (and optionally subcategories) under a Wikipedia category.\"\"\"\n",
    "    members = []\n",
    "    cmcontinue = \"\"\n",
    "    while True:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"categorymembers\",\n",
    "            \"cmtitle\": f\"Category:{category_title}\",\n",
    "            \"cmlimit\": \"max\",\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        if cmcontinue:\n",
    "            params[\"cmcontinue\"] = cmcontinue\n",
    "\n",
    "        query = baseurl + \"?\" + urllib.parse.urlencode(params)\n",
    "        req = urllib.request.Request(query, headers={\"User-Agent\": user_agent})\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            data = json.loads(response.read().decode(\"utf-8\"))\n",
    "\n",
    "        pages = data.get(\"query\", {}).get(\"categorymembers\", [])\n",
    "        for p in pages:\n",
    "            ns = p.get(\"ns\")\n",
    "            title = p.get(\"title\")\n",
    "            if ns == 0:\n",
    "                members.append(title)\n",
    "            elif ns == 14 and depth > 0:\n",
    "                print(f\"→ Exploring subcategory: {title}\")\n",
    "                members += get_category_members(title.replace(\"Category:\", \"\"), depth - 1)\n",
    "\n",
    "        if \"continue\" in data:\n",
    "            cmcontinue = data[\"continue\"][\"cmcontinue\"]\n",
    "            time.sleep(0.3)\n",
    "        else:\n",
    "            break\n",
    "    return members\n",
    "\n",
    "\n",
    "# --- Fetch full page text ---\n",
    "def fetch_wikitext(title):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    query = baseurl + \"?\" + urllib.parse.urlencode(params)\n",
    "    req = urllib.request.Request(query, headers={\"User-Agent\": user_agent})\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        data = json.loads(response.read().decode(\"utf-8\"))\n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    page = next(iter(pages.values()))\n",
    "    try:\n",
    "        return page[\"revisions\"][0][\"slots\"][\"main\"][\"*\"]\n",
    "    except (KeyError, IndexError):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63984fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"Association football terminology\"\n",
    "pages = get_category_members(category, depth=1)  # include subcategories too\n",
    "\n",
    "print(f\"Found {len(pages)} pages in category '{category}'.\")\n",
    "\n",
    "for title in pages:\n",
    "    save_txt(title, save_dir)\n",
    "    time.sleep(0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
